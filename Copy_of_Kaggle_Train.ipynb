{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Kaggle Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdI8ASGlbEgF3/3SozOxsM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hameon4/Kaggle-2022-GAN/blob/main/Copy_of_Kaggle_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 1: BUILD MODEL USING THE VALIDATION AND SOLUTION DATASET**"
      ],
      "metadata": {
        "id": "6fFNyhm9tuiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Sr6BBz0neesj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the training data\n",
        "df = pd.read_csv('/content/Train.csv')"
      ],
      "metadata": {
        "id": "xTWDWcIwbA7I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign variables for training while disregarding the Label column\n",
        "X = df.drop(columns = ['Label']).copy().values\n",
        "y = df['Label'].values"
      ],
      "metadata": {
        "id": "kemlZvcLbLsK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "\n",
        "# Range all values between -1 and 1\n",
        "x_train = x_train / 272070 * 2 - 1\n",
        "x_test = x_test / 272070 * 2 - 1\n",
        "\n",
        "N, D = df.values.shape"
      ],
      "metadata": {
        "id": "ETEfraV3uiLG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the dimensionality of the latent space\n",
        "latent_dim = 100"
      ],
      "metadata": {
        "id": "81SG1megvmin"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=500, criterion='entropy')\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJU8lRk5CxyW",
        "outputId": "3142ce6c-310a-4ed8-8bb7-abc78277b3ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesClassifier(criterion='entropy', n_estimators=500)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_valid = pd.read_csv('/content/Validation.csv')\n",
        "data_solution = pd.read_csv('/content/Solution.csv')\n",
        "\n",
        "X_valid = data_valid.drop(columns = ['ID']).copy()\n",
        "y_valid = data_solution.drop(columns = ['ID']).copy()"
      ],
      "metadata": {
        "id": "BeDSuQAt5Ij8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "p_test = model.predict(X_valid)\n",
        "rmse = mean_squared_error(y_valid, p_test, squared=False)\n",
        "print(f'RMSE: {round(rmse * 100, 3)}%');\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8y9K-VVjeWh",
        "outputId": "9fb8b864-9781-49a3-80cc-9ede0358afbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but ExtraTreesClassifier was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 10.761%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 2: UTILIZE GAN TO GENERATE SYNTHETIC DATA**"
      ],
      "metadata": {
        "id": "X-re99ectj5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/Train.csv')"
      ],
      "metadata": {
        "id": "8WOVnCq5ttKt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Model \n",
        "def build_generator(latent_dim):\n",
        "  i = Input(shape=(latent_dim, ))\n",
        "  x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Dense(D, activation='tanh')(x) # use tanh cuz we centered our data b/w -1 and 1\n",
        "\n",
        "  model = Model(i, x)\n",
        "  return model"
      ],
      "metadata": {
        "id": "RKFwDr3TrGte"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discrminator Model \n",
        "def build_discriminator(img_size):\n",
        "  i = Input(shape=(img_size,))\n",
        "  x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
        "  x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n",
        "  x = Dense(1, activation='sigmoid')(x) # sigmoid cuz binary classification\n",
        "  \n",
        "  model = Model(i, x)\n",
        "  return model"
      ],
      "metadata": {
        "id": "8Xo9WdGtrG1n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile both models in preparation for training\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator(D)\n",
        "discriminator.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(0.0002, 0.5),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build and compile the combined model \n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "# Create and input to represent noise sample from latent space\n",
        "z = Input(shape=(latent_dim))\n",
        "\n",
        "# Pass noise through generator to get an image\n",
        "img = generator(z)\n",
        "\n",
        "# Make sure only the generator is trained\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The true output is fake, but we label them real\n",
        "fake_pred = discriminator(img)\n",
        "\n",
        "# Create the combined model object\n",
        "combined_model = Model(z, fake_pred)\n",
        "\n",
        "# Compile the combined model\n",
        "combined_model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(0.002, 0.5)\n",
        ")"
      ],
      "metadata": {
        "id": "iGYV5deKrG7D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the GAN\n",
        "\n",
        "# Config\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "sample_period = 10 # every 'sample_period' step generates and saves some data \n",
        "\n",
        "# Create batch labels to use when calling train_on_batch\n",
        "ones = np.ones(batch_size)\n",
        "zeros = np.zeros(batch_size)\n",
        "\n",
        "# Store the losses\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "# Create a folder to store generated images\n",
        "# if not os.path.exists('gan_images'):\n",
        "#   os.makedirs('gan_images')"
      ],
      "metadata": {
        "id": "SkVCHtVerG-d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training loop\n",
        "for epoch in range(epochs):\n",
        "  ###########################\n",
        "  ### Train Discriminator ###\n",
        "  ###########################\n",
        "\n",
        "  # Select a random batch of images\n",
        "  idx = np.random.randint(0, df.values.shape[0], batch_size)\n",
        "  real_imgs = df.values[idx]\n",
        "\n",
        "  # Generate fake images\n",
        "  noise = np.random.randn(batch_size, latent_dim)\n",
        "  fake_imgs = generator.predict(noise)\n",
        "\n",
        "  # Train the discriminator\n",
        "  # both loss and accuracy are returned\n",
        "  d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
        "  d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
        "  d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "  d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
        "\n",
        "  ###########################\n",
        "  ### Train Generator ###\n",
        "  ###########################  \n",
        "\n",
        "  noise = np.random.randn(batch_size, latent_dim)\n",
        "  g_loss = combined_model.train_on_batch(noise, ones)\n",
        "\n",
        "  # Save the losses\n",
        "  d_losses.append(d_loss)\n",
        "  g_losses.append(g_loss)\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'epoch: {epoch + 1}/{epochs}, d_loss: {d_loss: .2f},\\\n",
        "    d_acc: {d_acc: .2f}, g_loss: {g_loss:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj4WQpXZrHB4",
        "outputId": "bcf5d15a-fa2b-4348-ab3a-fde17c979747"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1/100, d_loss:  1.27,    d_acc:  0.53, g_loss: 0.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = data['Label']\n",
        "z = 0\n",
        "o = 0\n",
        "\n",
        "for i in x:\n",
        "  if i == 0:\n",
        "    z += 1\n",
        "  elif i == 1:\n",
        "    o += 1\n",
        "print(f'Zeros: {z} Ones: {o}')\n",
        "diff = z - o\n",
        "\n",
        "lst = []\n",
        "while len(lst) <= diff-1:\n",
        "  z = np.random.randn(1, latent_dim)\n",
        "  for i in range(1):\n",
        "    genz = generator.predict(z)\n",
        "    lab = genz[:, -1]\n",
        "    if lab[i] > 0 and lab[i] <= 1 and np.isfinite() and np.nan:\n",
        "      lst.append(genz)\n",
        "    else:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwDBz3EFRbsS",
        "outputId": "f90b646f-e9bf-460a-b74d-67d082ba9071"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zeros: 3000 Ones: 1465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# merge fake with original\n",
        "synthetic_data = np.reshape(lst, (1535, 129))\n",
        "synthetic_data = pd.DataFrame(synthetic_data)\n",
        "malware_data = data.append(synthetic_data)"
      ],
      "metadata": {
        "id": "Ls_T_NuF_1q8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle synthetic data with original data\n",
        "shuffled_malware_data = malware_data.sample(frac=1)"
      ],
      "metadata": {
        "id": "kYgth4V1_5BA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = malware_data.drop(columns = ['Label']).copy()\n",
        "y = malware_data['Label']"
      ],
      "metadata": {
        "id": "h27AfBieAG4E"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.nan_to_num(X)\n",
        "y = np.nan_to_num(y)"
      ],
      "metadata": {
        "id": "EAfie1sroilm"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=500, criterion='entropy')\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr08tXhEAfun",
        "outputId": "8befb5c2-c8f0-4f61-e0a7-9cd909f313b9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesClassifier(criterion='entropy', n_estimators=500)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-8VAlkKr6s7",
        "outputId": "2ae4dbb2-176c-453f-c45b-41dedc70ef92"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 257)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_valid = pd.read_csv('/content/Validation.csv')\n",
        "data_solution = pd.read_csv('/content/Solution.csv')\n",
        "\n",
        "X_valid = data_valid.drop(columns = ['ID']).copy()\n",
        "y_valid = data_solution.drop(columns = ['ID']).copy()"
      ],
      "metadata": {
        "id": "3x7EBS3TAmOh"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "p_test = model.predict(X_valid)\n",
        "rmse_gan = mean_squared_error(y_valid, p_test, squared=False)\n",
        "print(f'RMSE: {round(rmse_gan * 100, 7)}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "KOLs1Z5PAvTB",
        "outputId": "c176aeaa-02bb-4965-93d8-d634d85cc88d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but ExtraTreesClassifier was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-6b654bd7148f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mp_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrmse_gan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'RMSE: {round(rmse_gan * 100, 7)}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    577\u001b[0m         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n\u001b[1;32m    578\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No support for np.int64 index based sparse matrices\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             raise ValueError(\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: X has 128 features, but ExtraTreesClassifier is expecting 257 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delta = round(rmse_gan - rmse, 3)\n",
        "print(delta)"
      ],
      "metadata": {
        "id": "Xeie3to-RJbW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}