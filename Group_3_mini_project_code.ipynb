{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hameon4/Kaggle-2022-GAN/blob/main/Group_3_mini_project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fFNyhm9tuiK"
      },
      "source": [
        "# **PART 1: BUILD MODEL USING THE VALIDATION AND SOLUTION DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Sr6BBz0neesj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xTWDWcIwbA7I"
      },
      "outputs": [],
      "source": [
        "# Load in the training data\n",
        "df = pd.read_csv('Train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kemlZvcLbLsK"
      },
      "outputs": [],
      "source": [
        "# Assign variables for training while disregarding the Label column\n",
        "X = df.drop(columns = ['Label']).copy().values\n",
        "y = df['Label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJU8lRk5CxyW",
        "outputId": "3646b43c-1f19-4d08-b053-1a308ee72962"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(criterion='entropy', n_estimators=500)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Build the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=500, criterion='entropy')\n",
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BeDSuQAt5Ij8"
      },
      "outputs": [],
      "source": [
        "# Load in the Validation and Solution datasets\n",
        "data_valid = pd.read_csv('Validation.csv')\n",
        "data_solution = pd.read_csv('Solution.csv')\n",
        "\n",
        "X_valid = data_valid.drop(columns = ['ID']).copy()\n",
        "y_valid = data_solution.drop(columns = ['ID']).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8y9K-VVjeWh"
      },
      "outputs": [],
      "source": [
        "# Acquire the baseline RMSE via validation set\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "p_test = model.predict(X_valid)\n",
        "rmse = mean_squared_error(y_valid, p_test, squared=False)\n",
        "print(f'RMSE: {round(rmse * 100, 7)}%');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-re99ectj5o"
      },
      "source": [
        "# **PART 2: UTILIZE GAN TO GENERATE SYNTHETIC DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8WOVnCq5ttKt"
      },
      "outputs": [],
      "source": [
        "# Load in the dataset\n",
        "import pandas as pd\n",
        "data = pd.read_csv('Train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gmCCJr5gr_8K"
      },
      "outputs": [],
      "source": [
        "# OOP class for our GAN\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "class SPARGAN():\n",
        "    \n",
        "    def __init__(self, latent_dim=100, hidden_layers=[512, 256], d_in = 129): \n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_in = d_in\n",
        "        \n",
        "    \n",
        "    def layer(x, layer_size):\n",
        "        x = Dense(layer_size, activation=LeakyReLU(alpha=0.2))(x)\n",
        "        x = BatchNormalization(momentum=0.8)(x)\n",
        "        x = Dropout(0.75)(x)\n",
        "        return x \n",
        "    \n",
        "        \n",
        "    # Generator Model\n",
        "    def build_generator(self):\n",
        "        input_layer = Input(shape=(self.latent_dim,))\n",
        "        x = input_layer\n",
        "        \n",
        "        for layer_size in self.hidden_layers:\n",
        "            x = SPARGAN.layer(x, layer_size)\n",
        "            \n",
        "        x = Dense(self.d_in, activation='relu')(x) \n",
        "\n",
        "        model = Model(input_layer, x)\n",
        "        self.generator = model\n",
        "\n",
        "    # Discrminator Model \n",
        "    def build_discriminator(self):\n",
        "        input_layer = Input(shape=(self.d_in,))\n",
        "        x = input_layer\n",
        "        for layer_size in reversed(self.hidden_layers):\n",
        "            x = SPARGAN.layer(x, layer_size)\n",
        "            \n",
        "        x = Dense(1, activation='sigmoid')(x) # sigmoid - binary classification\n",
        "\n",
        "        model = Model(input_layer, x)\n",
        "        self.discriminator = model\n",
        "        \n",
        "    def synthesize(self, diff):\n",
        "        lst = []\n",
        "        while len(lst) <= diff-1:\n",
        "            z = np.random.randn(100, self.latent_dim)\n",
        "            genz = self.generator.predict(z)\n",
        "            for i in range(len(z)):\n",
        "                lab = genz[i, -1]\n",
        "                if lab > 0 and np.isfinite(lab):\n",
        "                    if len(lst) <= diff-1:\n",
        "                        genz[i, -1] = 1\n",
        "                        lst.append(genz[i, :])\n",
        "                    else:\n",
        "                        break\n",
        "                        \n",
        "        np.reshape(lst, (1535, 129)).shape\n",
        "        \n",
        "        # merge fake with original\n",
        "        synthetic_data = np.reshape(lst, (1535, 129))\n",
        "        synthetic_data = pd.DataFrame(synthetic_data, columns = [f'F{i}' for i in range(1, 129)]+[\"Label\"])\n",
        "        return synthetic_data\n",
        "    \n",
        "    #fit model\n",
        "    def fit(self, df, batch_size=32, epochs=100, plot_loss=False):\n",
        "        # Compile both models in preparation for training\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.build_discriminator()\n",
        "        self.discriminator.compile(\n",
        "            loss='binary_crossentropy',\n",
        "            optimizer=Adam(0.0002, 0.5),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Build and compile the combined model \n",
        "        self.build_generator()\n",
        "\n",
        "        # Create and input to represent noise sample from latent space\n",
        "        z = Input(shape=(self.latent_dim))\n",
        "\n",
        "        # Pass noise through generator to get an image\n",
        "        row = self.generator(z)\n",
        "\n",
        "        # Make sure only the generator is trained\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The true output is fake, but we label them real\n",
        "        fake_pred = self.discriminator(row)\n",
        "\n",
        "        # Create the combined model object\n",
        "        combined_model = Model(z, fake_pred)\n",
        "\n",
        "        # Compile the combined model\n",
        "        combined_model.compile(\n",
        "            loss='binary_crossentropy',\n",
        "            optimizer=Adam(0.002, 0.5)\n",
        "        )\n",
        "\n",
        "        # Train the GAN \n",
        "\n",
        "        # Create batch labs to use when calling train_on_batch\n",
        "        ones = np.ones(batch_size)\n",
        "        zeros = np.zeros(batch_size)\n",
        "\n",
        "        # Store the losses\n",
        "        d_losses = []\n",
        "        g_losses = []\n",
        "\n",
        "        # Main training loop\n",
        "        for epoch in trange(epochs):\n",
        "            ###########################\n",
        "            ### Train Discriminator ###\n",
        "            ###########################\n",
        "\n",
        "            # Select a random batch of images\n",
        "            idx = np.random.randint(0, df.values.shape[0], batch_size)\n",
        "            real_rows = df.values[idx]\n",
        "\n",
        "            # Generate fake images\n",
        "            noise = np.random.randn(batch_size, self.latent_dim)\n",
        "            fake_rows = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            # both loss and accuracy are returned\n",
        "            d_loss_real, d_acc_real = self.discriminator.train_on_batch(real_rows, ones)\n",
        "            d_loss_fake, d_acc_fake = self.discriminator.train_on_batch(fake_rows, zeros)\n",
        "            d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "            d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
        "\n",
        "            ###########################\n",
        "            ##### Train Generator #####\n",
        "            ###########################  \n",
        "\n",
        "            noise = np.random.randn(batch_size, self.latent_dim)\n",
        "            g_loss = combined_model.train_on_batch(noise, ones)\n",
        "\n",
        "            # Save the losses\n",
        "            d_losses.append(d_loss)\n",
        "            g_losses.append(g_loss)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'epoch: {epoch + 1}/{epochs}, d_loss: {d_loss: .2f}, d_acc: {d_acc: .2f}, g_loss: {g_loss:.2f}')\n",
        "\n",
        "        \n",
        "        if plot_loss:\n",
        "            plt.plot(d_losses, label='discriminator_loss') \n",
        "            plt.plot(g_losses, label='generator_loss')\n",
        "            plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vVb7Ushr_8N"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model = SPARGAN()\n",
        "model.fit(data, plot_loss=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rco-IuWbr_8O",
        "outputId": "b6e18ace-ffeb-4496-c507-c73606ebf79f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zeros: 3000 Ones: 1465\n"
          ]
        }
      ],
      "source": [
        "# Count the number of ones and zeroes\n",
        "zeroes, ones = data['Label'].value_counts()\n",
        "print(f'Zeros: {zeroes} Ones: {ones}')\n",
        "diff = np.abs(zeroes - ones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "z6XOXZaur_8O"
      },
      "outputs": [],
      "source": [
        "# Append synthetic data to the original data\n",
        "synthetic_data = model.synthesize(diff)\n",
        "malware_data = data.append(synthetic_data)\n",
        "X = malware_data.drop(columns = ['Label']).copy()\n",
        "y = malware_data['Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr08tXhEAfun"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=500, criterion='entropy')\n",
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3x7EBS3TAmOh"
      },
      "outputs": [],
      "source": [
        "# Load validation and solution dataset\n",
        "data_valid = pd.read_csv('Validation.csv')\n",
        "data_solution = pd.read_csv('Solution.csv')\n",
        "\n",
        "X_valid = data_valid.drop(columns = ['ID']).copy()\n",
        "y_valid = data_solution.drop(columns = ['ID']).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOLs1Z5PAvTB"
      },
      "outputs": [],
      "source": [
        "# Acquire the new RMSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "p_test = model.predict(X_valid)\n",
        "rmse_gan = mean_squared_error(y_valid, p_test, squared=False)\n",
        "print(f'RMSE: {round(rmse_gan * 100, 7)}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xeie3to-RJbW"
      },
      "outputs": [],
      "source": [
        "# Calculate the RMSE's delta\n",
        "delta = round(rmse_gan - rmse, 3)\n",
        "print(abs(delta))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Group_3_mini_project_code.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}